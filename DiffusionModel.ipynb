{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "def load_data(data_dir):\n",
    "    all_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            df = pd.read_csv(file_path, skipinitialspace=False)\n",
    "            df = df[df[' Label'] != 'BENIGN']  #Removing rows with 'BENIGN' label and dropping label column\n",
    "            df = df.drop(' Label', axis=1)   \n",
    "            all_data.append(df)\n",
    "\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    logging.info(f\"Initial data shape: {combined_data.shape}\")\n",
    "\n",
    "    #replacing non-numeric values with NaN\n",
    "    for col in combined_data.columns:\n",
    "        combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce')\n",
    "    \n",
    "    #replacing infinite values with Nan\n",
    "    combined_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    combined_data = combined_data.dropna()\n",
    "    logging.info(f\"Data shape after cleaning: {combined_data.shape}\")\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    column_names = data.columns\n",
    "    column_types = data.dtypes\n",
    "    \n",
    "    # Clip the data to handle extreme values (1st to 99th percentile)\n",
    "    data = data.clip(lower=data.quantile(0.01), upper=data.quantile(0.99), axis=1)\n",
    "    \n",
    "    #ensure outputs are non-negative\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(data)\n",
    "    \n",
    "    return scaled_features, scaler, column_names, column_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  #ensure output is in [0,1] to match MinMax scaling\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to prevent overfitting\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(inputs, noise_factor=0.05):\n",
    "    #Adds random noise to the inputs for data augmentation\n",
    "    noisy_inputs = inputs + noise_factor * torch.randn_like(inputs)\n",
    "    return torch.clamp(noisy_inputs, 0.0, 1.0)  #ensures still in [0, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=50, device='cpu'):\n",
    "    criterion = nn.MSELoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.0001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch_x = batch[0].to(device)  #load batch to device\n",
    "\n",
    "            #Adding noise\n",
    "            noisy_batch_x = add_noise(batch_x)\n",
    "\n",
    "            #forward pass\n",
    "            outputs = model(noisy_batch_x)\n",
    "            loss = criterion(outputs, batch_x)\n",
    "\n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        #validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch_x = batch[0].to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_x)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        logging.info(f\"Epoch [{epoch+1}/{epochs}], Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "      \n",
    "        scheduler.step()\n",
    "\n",
    "        #check early stopping condition\n",
    "        early_stopping(avg_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            logging.info(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    #saving model\n",
    "    torch.save(model.state_dict(), 'diffusion_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate new samples\n",
    "def generate_samples(model, scaler, column_names, column_types, num_samples=1000, device='cpu'):\n",
    "    model.eval() \n",
    "    input_dim = len(column_names)\n",
    "    with torch.no_grad():\n",
    "        samples = torch.randn(num_samples, input_dim).to(device)\n",
    "        generated_samples = model(samples).cpu().numpy()\n",
    "    \n",
    "    #reverse scaling using inverse transform\n",
    "    generated_samples = scaler.inverse_transform(generated_samples)\n",
    "    \n",
    "    #preserve column types\n",
    "    generated_df = pd.DataFrame(generated_samples, columns=column_names)\n",
    "    for col, col_type in zip(column_names, column_types):\n",
    "        generated_df[col] = generated_df[col].astype(col_type)\n",
    "    \n",
    "    return generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initial data shape: (557646, 78)\n",
      "INFO:root:Data shape after cleaning: (556556, 78)\n",
      "/home/marif_umassd_edu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/marif_umassd_edu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "INFO:root:Epoch [1/50], Avg Train Loss: 0.0051, Avg Val Loss: 0.0006\n",
      "INFO:root:Epoch [2/50], Avg Train Loss: 0.0017, Avg Val Loss: 0.0005\n",
      "INFO:root:Epoch [3/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0006\n",
      "INFO:root:Epoch [4/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0005\n",
      "INFO:root:Epoch [5/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0005\n",
      "INFO:root:Epoch [6/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0005\n",
      "INFO:root:Epoch [7/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0005\n",
      "INFO:root:Epoch [8/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0005\n",
      "INFO:root:Epoch [9/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0005\n",
      "INFO:root:Epoch [10/50], Avg Train Loss: 0.0016, Avg Val Loss: 0.0006\n",
      "INFO:root:Epoch [11/50], Avg Train Loss: 0.0014, Avg Val Loss: 0.0004\n",
      "INFO:root:Epoch [12/50], Avg Train Loss: 0.0014, Avg Val Loss: 0.0004\n",
      "INFO:root:Early stopping triggered\n",
      "INFO:root:Generated data saved to diffusion_model_results.csv\n"
     ]
    }
   ],
   "source": [
    "def main(data_dir, output_file, epochs=50, batch_size=64, num_samples=1000):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    data = load_data(data_dir)\n",
    "    scaled_features, scaler, column_names, column_types = preprocess_data(data)\n",
    "\n",
    "    #split data into train and validation sets\n",
    "    dataset = TensorDataset(torch.tensor(scaled_features, dtype=torch.float32))\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    #Create DataLoader for batch processing\n",
    "    num_workers = min(4, multiprocessing.cpu_count() - 1)  # Use a safe number of workers\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    input_dim = scaled_features.shape[1]\n",
    "    model = DiffusionModel(input_dim).to(device)\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, epochs, device)\n",
    "\n",
    "    #generate samples and save them to a CSV file\n",
    "    generated_samples = generate_samples(model, scaler, column_names, column_types, num_samples, device)\n",
    "    generated_samples.to_csv(output_file, index=False)\n",
    "    logging.info(f\"Generated data saved to {output_file}\")\n",
    "\n",
    "#parameters\n",
    "data_dir = 'gan_dataset'  #directory containing all CSV files\n",
    "output_file = 'diffusion_model_results.csv'\n",
    "epochs = 50  \n",
    "batch_size = 64 \n",
    "num_samples = 1000  #Number of generated samples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(data_dir, output_file, epochs, batch_size, num_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
